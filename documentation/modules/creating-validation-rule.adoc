// Module included in the following assemblies:
//
// * documentation/doc-Migration_Toolkit_for_Virtualization/master.adoc

:_content-type: PROCEDURE
[id="creating-validation-rule_{context}"]
= Creating a validation rule

[role="_abstract"]

To ensure that your custom validation rules persist across pod restarts, scaling events, and {project-first} upgrades, deploy the rules by using a *ConfigMap* and reference the rules in the {project-short} Custom Resource (CR).

[IMPORTANT]
====
* If you create a rule with the same _name_ as an existing rule, the `Validation` service performs an `OR` operation with the rules.
* If you create a rule that contradicts a default rule, the `Validation` service will not start.
====

.Validation rule example

Validation rules are based on virtual machine (VM) attributes that are collected and simplified by the Provider Inventory service.

The `Provider Inventory` service acts as an abstraction layer, normalizing complex, provider-specific VM properties into standardized, testable attributes for the validation engine. This allows rules to be written once and applied across different source environments.

For example, a validation rule needs to check if a VMware VM has NUMA node affinity configured.

* Provider-Specific Path: The raw VMware API uses a deeply nested path to expose this configuration: `MOR:VirtualMachine.config.extraConfig["numa.nodeAffinity"]`.

* Simplified Inventory Attribute: The `Provider Inventory` service simplifies this internal path and presents it as a testable attribute with a normalized list value:

The value for this attribute is standardized as a list.

[cols=",",options="header",]
|===
|Inventory Attribute
|Example Value

|`numa.nodeAffinity`
|`["True"]` or `[]` (empty list if not configured)
|===

This simplified attribute (`numa.nodeAffinity`) is what the validation engine uses to execute the rules efficiently.

Validation rules are based on virtual machine (VM) attributes collected by the `Provider Inventory` service.

For example, the VMware API uses this path to check whether a VMware VM has NUMA node affinity configured: `MOR:VirtualMachine.config.extraConfig["numa.nodeAffinity"]`.

The `Provider Inventory` service simplifies this configuration and returns a testable attribute with a list value:

[source,terminal]
----
"numaNodeAffinity": [
    "0",
    "1"
],
----

You create a link:https://www.openpolicyagent.org/docs/latest/policy-language/[Rego] query, based on this attribute, and add it to the `forklift-validation-config` config map:

[source,terminal]
----
`count(input.numaNodeAffinity) != 0`
----

.Procedure

. Create a config map CR according to the following example:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: <forklift-validation-config>
  namespace: {namespace}
data:
  vmware_multiple_disks.rego: |-
    package <provider_package> <1>

    has_multiple_disks { <2>
      count(input.disks) > 1
    }

    concerns[flag] {
      has_multiple_disks <3>
        flag := {
          "category": "<Information>", <4>
          "label": "Multiple disks detected",
          "assessment": "Multiple disks detected on this VM."
        }
    }
EOF
----
<1> Specify the provider package name. Allowed values are `io.konveyor.forklift.vmware` for VMware and `io.konveyor.forklift.ovirt` for {rhv-full}.
<2> Specify the `concerns` name and Rego query.
<3> Specify the `concerns` name and `flag` parameter values.
<4> Allowed values are `Critical`, `Warning`, and `Information`.

. Stop the `Validation` pod by scaling the `forklift-controller` deployment to `0`:
+
[source,terminal,subs="attributes+"]
----
$ {oc} scale -n {namespace} --replicas=0 deployment/forklift-validation
----

. Start the `Validation` pod by scaling the `forklift-controller` deployment to `1`:
+
[source,terminal,subs="attributes+"]
----
$ {oc} scale -n openshift-mtv --replicas=1 deployment/forklift-validation
----
. Wait a few moments for the pod to terminate.

. Check the `Validation` pod log to verify that the pod started:
+
[source,terminal,subs="attributes+"]
----
$ {oc} logs -f <validation_pod>
----
+
If the custom rule conflicts with a default rule, the `Validation` pod will not start.

. Remove the source provider:
+
[source,terminal,subs="attributes+"]
----
$ {oc} delete provider <provider> -n {namespace}
----

. Add the source provider to apply the new rule:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Provider
metadata:
  name: <provider>
  namespace: {namespace}
spec:
  type: <provider_type> <1>
  url: <api_end_point> <2>
  secret:
    name: <secret> <3>
    namespace: {namespace}
EOF
----
<1> Allowed values are `ovirt`, `vsphere`, and `openstack`.
<2> Specify the API end point URL, for example, `https://<vCenter_host>/sdk` for vSphere, `https://<engine_host>/ovirt-engine/api` for {rhv-short}, or `https://<identity_service>/v3` for {osp}.
<3> Specify the name of the provider `Secret` CR.

You must update the rules version after creating a custom rule so that the `Inventory` service detects the changes and validates the VMs.
