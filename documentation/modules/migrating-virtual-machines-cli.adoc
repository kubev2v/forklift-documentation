// Module included in the following assemblies:
//
// * documentation/doc-Migration_Toolkit_for_Virtualization/master.adoc

[id="migrating-virtual-machines-cli_{context}"]
= Migrating virtual machines

You migrate virtual machines (VMs) from the command line (CLI) by creating {project-short} custom resources (CRs).

[IMPORTANT]
====
You must specify a name for cluster-scoped CRs.

You must specify both a name and a namespace for namespace-scoped CRs.
====

As a Technology Preview, {project-short} supports migrations using OpenStack source providers.

[IMPORTANT]
====
Migration using OpenStack source providers is a Technology Preview feature only. Technology Preview features
are not supported with Red Hat production service level agreements (SLAs) and
might not be functionally complete. Red Hat does not recommend using them
in production. These features provide early access to upcoming product
features, enabling customers to test functionality and provide feedback during
the development process.

For more information about the support scope of Red Hat Technology Preview
features, see https://access.redhat.com/support/offerings/techpreview/.
====

[NOTE]
====
Migration using {osp} source providers only supports VMs that use only Cinder volumes.
====

.Prerequisites

* You must be logged in as a user with `cluster-admin` privileges.
* VMware only: You must have a VMware Virtual Disk Development Kit (VDDK) image in a secure registry that is accessible to all clusters.

.Procedure

. Create a `Secret` manifest for the source provider credentials:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: <secret>
  namespace: {namespace}
  ownerReferences: <1>
    - apiVersion: forklift.konveyor.io/v1beta1
      kind: Provider
      name: <provider_name>
      uid: <provider_uid>
  labels:
    createdForProviderType: <provider_type> <2>
type: Opaque
stringData:
  user: <user> <3>
  password: <password> <4>
  domainName: <domain_name> <5>
  projectName: <project_name> <6>
  regionName: <region name> <7>
  cacert: | <8>
    <ca_certificate>
  url: <api_end_point> <9>
  thumbprint: <vcenter_fingerprint> <10>
----
<1> The `ownerReferences` section is optional.
<2>  Specify the type of source provider. Allowed values are `ovirt`, `vsphere`, and `openstack`. This label is needed to verify the credentials are correct when the remote system is accessible and, for {rhv-short}, to retrieve the {manager} CA certificate when a third-party certificate is specified.
<3> Specify the vCenter user, the {rhv-short} {manager} user, or the {osp} user.
<4> Specify the user password.
<5> {osp} only: Specify the domain name.
<6> {osp} only: Specify the project name.
<7> {osp} only: Specify the name of the {osp} region.
<8> {rhv-short} and {osp} only: For {rhv-short}, enter the {manager} CA certificate unless it was replaced by a third-party certificate, in which case enter the {manager} Apache CA certificate. You can retrieve the {manager} CA certificate at https://<engine_host>/ovirt-engine/services/pki-resource?resource=ca-certificate&format=X509-PEM-CA. For {osp}, enter the CA certificate for connecting to the source environment.
<9> Specify the API end point URL, for example, `https://<vCenter_host>/sdk` for vSphere, `https://<engine_host>/ovirt-engine/api/` for {rhv-short}, or `https://<identity_service>/v3` for {osp}.
<10> VMware only: Specify the vCenter SHA-1 fingerprint.

. Create a `Provider` manifest for the source provider:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Provider
metadata:
  name: <provider>
  namespace: {namespace}
spec:
  type: <provider_type> <1>
  url: <api_end_point> <2>
  settings:
    vddkInitImage: <registry_route_or_server_path>/vddk:<tag> <3>
  secret:
    name: <secret> <4>
    namespace: {namespace}
EOF
----
<1> Allowed values are `ovirt`, `vsphere`, and `openstack`.
<2> Specify the API end point URL, for example, `https://<vCenter_host>/sdk` for vSphere, `https://<engine_host>/ovirt-engine/api/` for {rhv-short}, or `https://<identity_service>/v3` for {osp}.
<3> VMware only: Specify the VDDK image that you created.
<4> Specify the name of provider `Secret` CR.

. VMware only: Create a `Host` manifest:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Host
metadata:
  name: <vmware_host>
  namespace: {namespace}
spec:
  provider:
    namespace: {namespace}
    name: <source_provider> <1>
  id: <source_host_mor> <2>
  ipAddress: <source_network_ip> <3>
EOF
----
<1> Specify the name of the VMware `Provider` CR.
<2> Specify the managed object reference (MOR) of the VMware host.
<3> Specify the IP address of the VMware migration network.

. Create a `NetworkMap` manifest to map the source and destination networks:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: NetworkMap
metadata:
  name: <network_map>
  namespace: {namespace}
spec:
  map:
    - destination:
        name: <pod>
        namespace: {namespace}
        type: pod <1>
      source: <2>
        id: <source_network_id> <3>
        name: <source_network_name>
    - destination:
        name: <network_attachment_definition> <4>
        namespace: <network_attachment_definition_namespace> <5>
        type: multus
      source:
        id: <source_network_id>
        name: <source_network_name>
  provider:
    source:
      name: <source_provider>
      namespace: {namespace}
    destination:
      name: <destination_cluster>
      namespace: {namespace}
EOF
----
<1> Allowed values are `pod` and `multus`.
<2> You can use either the `id` _or_ the `name` parameter to specify the source network.
<3> Specify the VMware network MOR, the {rhv-short} network UUID, or the {osp} network UUID.
<4> Specify a network attachment definition for each additional {virt} network.
<5> Specify the namespace of the {virt} network attachment definition.

. Create a `StorageMap` manifest to map source and destination storage:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: StorageMap
metadata:
  name: <storage_map>
  namespace: {namespace}
spec:
  map:
    - destination:
        storageClass: <storage_class>
        accessMode: <access_mode> <1>
      source:
        id: <source_datastore> <2>
    - destination:
        storageClass: <storage_class>
        accessMode: <access_mode>
      source:
        id: <source_datastore>
  provider:
    source:
      name: <source_provider>
      namespace: {namespace}
    destination:
      name: <destination_cluster>
      namespace: {namespace}
EOF
----
<1> Allowed values are `ReadWriteOnce` and `ReadWriteMany`.
<2> Specify the VMware data storage MOR, the {rhv-short} storage domain UUID, or the {osp} `volume_type` UUID. For example, `f2737930-b567-451a-9ceb-2887f6207009`.

. Optional: Create a `Hook` manifest to run custom code on a VM during the phase specified in the `Plan` CR:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Hook
metadata:
  name: <hook>
  namespace: {namespace}
spec:
  image: quay.io/konveyor/hook-runner <1>
  playbook: | <2>
    LS0tCi0gbmFtZTogTWFpbgogIGhvc3RzOiBsb2NhbGhvc3QKICB0YXNrczoKICAtIG5hbWU6IExv
    YWQgUGxhbgogICAgaW5jbHVkZV92YXJzOgogICAgICBmaWxlOiAiL3RtcC9ob29rL3BsYW4ueW1s
    IgogICAgICBuYW1lOiBwbGFuCiAgLSBuYW1lOiBMb2FkIFdvcmtsb2FkCiAgICBpbmNsdWRlX3Zh
    cnM6CiAgICAgIGZpbGU6ICIvdG1wL2hvb2svd29ya2xvYWQueW1sIgogICAgICBuYW1lOiB3b3Jr
    bG9hZAoK
EOF
----
<1> You can use the default `hook-runner` image or specify a custom image. If you specify a custom image, you do not have to specify a playbook.
<2> Optional: Base64-encoded Ansible playbook. If you specify a playbook, the `image` must be `hook-runner`.

. Create a `Plan` manifest for the migration:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Plan
metadata:
  name: <plan> <1>
  namespace: {namespace}
spec:
  warm: true <2>
  provider:
    source:
      name: <source_provider>
      namespace: {namespace}
    destination:
      name: <destination_cluster>
      namespace: {namespace}
  map:
    network: <3>
      name: <network_map> <4>
      namespace: {namespace}
    storage:
      name: <storage_map> <5>
      namespace: {namespace}
  targetNamespace: {namespace}
  vms: <6>
    - id: <source_vm> <7>
    - name: <source_vm>
      hooks: <8>
        - hook:
            namespace: {namespace}
            name: <hook> <9>
          step: <step> <10>
EOF
----
<1> Specify the name of the `Plan` CR.
<2> Specify whether the migration is warm or cold. If you specify a warm migration without specifying a value for the `cutover` parameter in the `Migration` manifest, only the precopy stage will run.
<3> You can add multiple network mappings.
<4> Specify the name of the `NetworkMap` CR.
<5> Specify the name of the `StorageMap` CR.
<6> You can use either the `id` _or_ the `name` parameter to specify the source VMs.
<7> Specify the VMware VM MOR, {rhv-short} VM UUID or the {osp} VM UUID.
<8> Optional: You can specify up to two hooks for a VM. Each hook must run during a separate migration step.
<9> Specify the name of the `Hook` CR.
<10> Allowed values are `PreHook`, before the migration plan starts, or `PostHook`, after the migration is complete.

. Create a `Migration` manifest to run the `Plan` CR:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Migration
metadata:
  name: <migration> <1>
  namespace: {namespace}
spec:
  plan:
    name: <plan> <2>
    namespace: {namespace}
  cutover: <cutover_time> <3>
EOF
----
<1> Specify the name of the `Migration` CR.
<2> Specify the name of the `Plan` CR that you are running. The `Migration` CR creates a `VirtualMachine` CR for each VM that is migrated.
<3> Optional: Specify a cutover time according to the ISO 8601 format with the UTC time offset, for example, `2021-04-04T01:23:45.678+09:00`.
+
You can associate multiple `Migration` CRs with a single `Plan` CR. If a migration does not complete, you can create a new `Migration` CR, without changing the `Plan` CR, to migrate the remaining VMs.

. Retrieve the `Migration` CR to monitor the progress of the migration:
+
[source,terminal,subs="attributes+"]
----
$ {oc} get migration/<migration> -n {namespace} -o yaml
----
