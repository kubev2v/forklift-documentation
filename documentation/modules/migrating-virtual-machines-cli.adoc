// Module included in the following assemblies:
//
// * documentation/doc-Migration_Toolkit_for_Virtualization/master.adoc

[id="migrating-virtual-machines-cli_{context}"]
= Migrating virtual machines from the command line interface

You can migrate virtual machines (VMs) from the command line (CLI) by creating the following custom resources (CRs):

* `Secret` contains the VMware provider credentials.
* `Provider` contains the VMware provider details.
* `Host` contains the VMware host details.
* `NetworkMap` maps the source and destination networks.
* `StorageMap` maps the source and destination storage.
* `Plan` contains a list of VMs to migrate and specifies whether the migration is cold or warm. The `Plan` references the providers and maps.
* `Migration` runs the `Plan`. If the migration is warm, it specifies the cutover time.
+
You can associate multiple `Migration` CRs with a single `Plan` CR. If a migration does not complete, you can create a new `Migration` CR, without changing the `Plan` CR, to migrate the remaining VMs.

[NOTE]
====
The API uses the term `destination` to refer to the target environment.
====

.Prerequisites

* You must be logged in as a user with `cluster-admin` privileges.
* The link:https://docs.openshift.com/container-platform/{ocp-version}/cli_reference/openshift_cli/getting-started-cli.html[OpenShift CLI] must be installed.
* If you are mapping more than one source and destination network, you must create a link:https://docs.openshift.com/container-platform/{ocp-version}/virt/virtual_machines/vm_networking/virt-attaching-vm-multiple-networks.html#virt-creating-network-attachment-definition[network attachment definition] for each additional destination network.
* The VDDK image must be added to the `v2v-vmware` config map.
* If you are performing a warm migration, you must enable link:https://kb.vmware.com/s/article/1020128[changed block tracking (CBT)] on the VMs and on the VM disks.
* If you are performing more than 10 concurrent migrations from a single ESXi host, you must increase the NFC service memory of the host.

.Procedure

. Obtain the SHA-1 fingerprint of the vCenter host:
+
[source,terminal]
----
$ openssl s_client \
    -connect <vcenter_host>:443 \ <1>
    < /dev/null 2>/dev/null \
    | openssl x509 -fingerprint -noout -in /dev/stdin \
    | cut -d '=' -f 2
----
<1> Specify the vCenter host name.
+
.Example output
+
[source,terminal]
----
01:23:45:67:89:AB:CD:EF:01:23:45:67:89:AB:CD:EF:01:23:45:67
----

. Create a `Secret` CR manifest for the VMware provider:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: vmware-secret
  namespace: {namespace}
type: Opaque
stringData:
  user: <user_name> <1>
  password: <password> <2>
  thumbprint: <fingerprint> <3>
EOF
----
<1> Specify the vCenter administrator account, for example, `administrator@vsphere.local`.
<2> Specify the vCenter password.
<3> Specify the SHA-1 fingerprint of the vCenter host.

. Create a `Provider` CR manifest for the VMware provider:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Provider
metadata:
  name: vmware-provider
  namespace: {namespace}
spec:
  type: vsphere
  url: <api_end_point> <1>
  secret:
    name: <vmware_secret> <2>
    namespace: {namespace}
EOF
----
<1> Specify the vSphere API end point, for example, `https://<vcenter.host.com>/sdk`.
<2> Specify the name of the VMware `Secret` CR.

. Create a `Host` CR manifest for the VMware host:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Host
metadata:
  name: vmware_host
  namespace: {namespace}
spec:
  provider:
    namespace: {namespace}
    name: <provider_name> <1>
  id: <source_host_mor> <2>
  ipAddress: <source_network> <3>
EOF
----
<1> Specify the name of the VMware `Provider` CR.
<2> Specify the _managed object reference_ of the VMware host.
<3> Specify the IP address of the VMware migration network.

. Create a `NetworkMap` CR manifest to map the source and destination networks:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: NetworkMap
metadata:
  name: <networkmap_name>
  namespace: {namespace}
spec:
  map:
    - destination:
        type: pod <1>
      source:
        id: <source_network_mor> <2>
    - destination:
        type: multus <1>
        name: <networkattachmentdefinition_name> <3>
      source:
        id: <source_network_mor> <2>
  provider:
    source:
      name: vmware-provider
      namespace: {namespace}
    destination:
      name: destination-cluster
      namespace: {namespace}
EOF
----
<1> Allowed values are `pod` and `multus`.
<2> Specify the managed object reference of the VMware network.
<3> Specify the network attachment definition for each additional destination network.

. Create a `StorageMap` CR manifest:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: StorageMap
metadata:
  name: <storagemap_name>
  namespace: {namespace}
spec:
  map:
    - destination:
        storageClass: <destination_storageclass>
      source:
        id: <source_datastore_mor> <1>
  provider:
    source:
      name: vmware-provider
      namespace: {namespace}
    destination:
      name: destination-cluster
      namespace: {namespace}
EOF
----
<1> Specify the managed object reference of the VMware data storage.

. Create a `Plan` CR manifest for the migration:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Plan
metadata:
  name: <plan_name> <1>
  namespace: {namespace}
spec:
  provider:
    source:
      name: vmware-provider
      namespace: {namespace}
    destination:
      name: destination-cluster
      namespace: {namespace}
  warm: true <2>
  map:
    network: <3>
      name: <networkmap_name> <4>
      namespace: {namespace}
    storage:
      name: <storagemap_name> <5>
      namespace: {namespace}
  targetNamespace: {namespace}
  vms: <6>
    - id: <source_vm_mor> <7>
    - name: <source_vm_name>
EOF
----
<1> Specify the name of the `Plan` CR.
<2> Specify whether the migration is warm or cold. If you specify a warm migration without specifying a value for the `cutover` parameter in the `Migration` CR manifest, only the precopy stage will run.
<3> You can create multiple network mappings for source and destination networks.
<4> Specify the name of the `NetworkMap` CR.
<5> Specify the name of the `StorageMap` CR.
<6> You can use either the `id` _or_ the `name` parameter to specify the source VMs.
<7> Specify the managed object reference of the VMware VM.

. Optional: To change the time interval between the CBT snapshots for warm migration, patch the `vm-import-controller-config` config map:
+
[source,terminal]
----
$ oc patch configmap/vm-import-controller-config \
  -n openshift-cnv -p '{"data": \
  {"warmImport.intervalMinutes": "<interval>"}}' <1>
----
<1> Specify the time interval in minutes. The default value is `60`.

. Create a `Migration` CR manifest to run the `Plan` CR:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Migration
metadata:
  name: <migration_name> <1>
  namespace: {namespace}
spec:
  plan:
    name: <plan_name> <2>
    namespace: {namespace}
  cutover: <cutover_time> <3>
EOF
----
<1> Specify the name of the `Migration` CR.
<2> Specify the name of the `Plan` CR that you are running. The `Migration` CR creates a `VirtualMachineImport` CR for each VM that is migrated.
<3> Optional. Specify a cutover time according to the ISO 8601 format with the UTC time offset, for example, `2021-04-04T01:23:45.678+09:00`.

. View the `VirtualMachineImport` pods to monitor the progress of the migration:
+
[source,terminal,subs="attributes+"]
----
$ oc get pods -n {namespace}
----
