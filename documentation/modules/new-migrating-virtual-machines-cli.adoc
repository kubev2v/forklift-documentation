// Module included in the following assemblies:
//
// * documentation/doc-Migration_Toolkit_for_Virtualization/master.adoc

:_mod-docs-content-type: PROCEDURE
[id="new-migrating-virtual-machines-cli_{context}"]

ifdef::vmware[]
= Migrating from a VMware vSphere source provider

You can migrate from a VMware vSphere source provider by using the command-line interface (CLI).

include::snip_anti-virus-warning.adoc[]

include::snip_no-nvme.adoc[]

[NOTE]
====
To migrate virtual machines (VMs) that have shared disks, see xref:mtv-shared-disks_{context}[Migrating virtual machines with shared disks].
====

endif::[]
ifdef::rhv[]
= Migrating from a {rhv-full} source provider

You can migrate from a {rhv-full} ({rhv-short}) source provider by using the command-line interface (CLI).

.Prerequisites

If you are migrating a virtual machine with a direct LUN disk, ensure that the nodes in the {virt} destination cluster that the VM is expected to run on can access the backend storage.

include::snip-migrating-luns.adoc[]

endif::[]
ifdef::ova[]
= Migrating from an Open Virtual Appliance (OVA) source provider

You can migrate from Open Virtual Appliance (OVA) files that were created by VMware vSphere as a source provider by using the command-line interface (CLI).

endif::[]
ifdef::ostack[]
= Migrating from an {osp} source provider

You can migrate from an {osp} source provider by using the command-line interface (CLI).
endif::[]
ifdef::cnv[]
= Migrating from a Red Hat {virt} source provider

You can use a Red Hat {virt} provider as either a source provider or as a destination provider. You can migrate from an {virt} source provider by using the command-line interface (CLI).

[NOTE]
====
The {ocp} cluster version of the source provider must be 4.16 or later.
====

endif::[]

.Procedure
. Create a `Secret` manifest for the source provider credentials:

ifdef::vmware[]
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: <secret>
  namespace: <namespace>
  ownerReferences: <1>
    - apiVersion: forklift.konveyor.io/v1beta1
      kind: Provider
      name: <provider_name>
      uid: <provider_uid>
  labels:
    createdForProviderType: vsphere
    createdForResourceType: providers
type: Opaque
stringData:
  user: <user> <2>
  password: <password> <3>
  insecureSkipVerify: <"true"/"false"> <4>
  cacert: | <5>
    <ca_certificate>
  url: <api_end_point> <6>
EOF
----
<1> The `ownerReferences` section is optional.
<2> Specify the vCenter user or the ESX/ESXi user.
<3> Specify the password of the vCenter user or the ESX/ESXi user.
<4> Specify `"true"` to skip certificate verification, and specify `"false"` to verify the certificate. Defaults to `"false"` if not specified. Skipping certificate verification proceeds with an insecure migration and then the certificate is not required. Insecure migration means that the transferred data is sent over an insecure connection and potentially sensitive data could be exposed.
<5> When this field is not set and 'skip certificate verification' is disabled, {project-short} attempts to use the system CA.
<6> Specify the API endpoint URL of the vCenter or the ESX/ESXi, for example, `https://<vCenter_host>/sdk`.
endif::[]
ifdef::rhv[]
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: <secret>
  namespace: <namespace>
  ownerReferences: <1>
    - apiVersion: forklift.konveyor.io/v1beta1
      kind: Provider
      name: <provider_name>
      uid: <provider_uid>
  labels:
    createdForProviderType: ovirt
    createdForResourceType: providers
type: Opaque
stringData:
  user: <user> <2>
  password: <password> <3>
  insecureSkipVerify: <"true"/"false"> <4>
  cacert: | <5>
    <ca_certificate>
  url: <api_end_point> <6>
EOF
----
<1> The `ownerReferences` section is optional.
<2> Specify the {rhv-short} {manager} user.
<3> Specify the user password.
<4> Specify `"true"` to skip certificate verification, and specify `"false"` to verify the certificate. Defaults to `"false"` if not specified. Skipping certificate verification proceeds with an insecure migration and then the certificate is not required. Insecure migration means that the transferred data is sent over an insecure connection and potentially sensitive data could be exposed.
<5> Enter the {manager} CA certificate, unless it was replaced by a third-party certificate, in which case, enter the {manager} Apache CA certificate. You can retrieve the {manager} CA certificate at https://<engine_host>/ovirt-engine/services/pki-resource?resource=ca-certificate&format=X509-PEM-CA.
<6> Specify the API endpoint URL, for example, `https://<engine_host>/ovirt-engine/api`.
endif::[]
ifdef::ova[]
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: <secret>
  namespace: <namespace>
  ownerReferences: <1>
    - apiVersion: forklift.konveyor.io/v1beta1
      kind: Provider
      name: <provider_name>
      uid: <provider_uid>
  labels:
    createdForProviderType: ova
    createdForResourceType: providers
type: Opaque
stringData:
  url: <nfs_server:/nfs_path> <2>
EOF
----
<1> The `ownerReferences` section is optional.
<2> where: `nfs_server` is an IP or hostname of the server where the share was created and `nfs_path` is the path on the server where the OVA files are stored.
endif::[]
ifdef::ostack[]
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: <secret>
  namespace: <namespace>
  ownerReferences: <1>
    - apiVersion: forklift.konveyor.io/v1beta1
      kind: Provider
      name: <provider_name>
      uid: <provider_uid>
  labels:
    createdForProviderType: openstack
    createdForResourceType: providers
type: Opaque
stringData:
  user: <user> <2>
  password: <password> <3>
  insecureSkipVerify: <"true"/"false"> <4>
  domainName: <domain_name>
  projectName: <project_name>
  regionName: <region_name>
  cacert: | <5>
    <ca_certificate>
  url: <api_end_point> <6>
EOF
----
<1> The `ownerReferences` section is optional.
<2> Specify the {osp} user.
<3> Specify the user {osp} password.
<4> Specify `"true"` to skip certificate verification, and specify `"false"` to verify the certificate. Defaults to `"false"` if not specified. Skipping certificate verification proceeds with an insecure migration and then the certificate is not required. Insecure migration means that the transferred data is sent over an insecure connection and potentially sensitive data could be exposed.
<5> When this field is not set and 'skip certificate verification' is disabled, {project-short} attempts to use the system CA.
<6> Specify the API endpoint URL, for example, `https://<identity_service>/v3`.
endif::[]

ifdef::cnv[]
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: <secret>
  namespace: <namespace>
  ownerReferences: <1>
    - apiVersion: forklift.konveyor.io/v1beta1
      kind: Provider
      name: <provider_name>
      uid: <provider_uid>
  labels:
    createdForProviderType: openshift
    createdForResourceType: providers
type: Opaque
stringData:
  token: <token> <2>
  password: <password> <3>
  insecureSkipVerify: <"true"/"false"> <4>
  cacert: | <5>
    <ca_certificate>
  url: <api_end_point> <6>
EOF
----
<1> The `ownerReferences` section is optional.
<2> Specify a token for a service account with `cluster-admin` privileges. If both `token` and `url` are left blank, the local {ocp-short} cluster is used.
<3> Specify the user password.
<4> Specify `"true"` to skip certificate verification, and specify `"false"` to verify the certificate. Defaults to `"false"` if not specified. Skipping certificate verification proceeds with an insecure migration and then the certificate is not required. Insecure migration means that the transferred data is sent over an insecure connection and potentially sensitive data could be exposed.
<5> When this field is not set and 'skip certificate verification' is disabled, {project-short} attempts to use the system CA.
<6> Specify the URL of the endpoint of the API server.
endif::[]

[start=2]
. Create a `Provider` manifest for the source provider:

ifdef::vmware[]
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Provider
metadata:
  name: <source_provider>
  namespace: <namespace>
spec:
  type: vsphere
  url: <api_end_point> <1>
  settings:
    vddkInitImage: <VDDK_image> <2>
    sdkEndpoint: vcenter <3>
  secret:
    name: <secret> <4>
    namespace: <namespace>
EOF
----
<1> Specify the URL of the API endpoint, for example, `https://<vCenter_host>/sdk`.
<2> Optional, but it is strongly recommended to create a VDDK image to accelerate migrations. Follow OpenShift documentation to specify the VDDK image you created.
<3> Options: `vcenter` or `esxi`.
<4> Specify the name of the provider `Secret` CR.
endif::[]

ifdef::rhv[]
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Provider
metadata:
  name: <source_provider>
  namespace: <namespace>
spec:
  type: ovirt
  url: <api_end_point> <1>
  secret:
    name: <secret> <2>
    namespace: <namespace>
EOF
----
<1> Specify the URL of the API endpoint, for example, `https://<engine_host>/ovirt-engine/api`.
<2> Specify the name of provider `Secret` CR.
endif::[]

ifdef::ova[]
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Provider
metadata:
  name: <source_provider>
  namespace: <namespace>
spec:
  type: ova
  url:  <nfs_server:/nfs_path> <1>
  secret:
    name: <secret> <2>
    namespace: <namespace>
EOF
----
<1>  where: `nfs_server` is an IP or hostname of the server where the share was created and `nfs_path` is the path on the server where the OVA files are stored.
<2> Specify the name of provider `Secret` CR.
endif::[]

ifdef::ostack[]
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Provider
metadata:
  name: <source_provider>
  namespace: <namespace>
spec:
  type: openstack
  url: <api_end_point> <1>
  secret:
    name: <secret> <2>
    namespace: <namespace>
EOF
----
<1> Specify the URL of the API endpoint.
<2> Specify the name of provider `Secret` CR.
endif::[]

ifdef::cnv[]
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Provider
metadata:
  name: <source_provider>
  namespace: <namespace>
spec:
  type: openshift
  url: <api_end_point> <1>
  secret:
    name: <secret> <2>
    namespace: <namespace>
EOF
----
<1> Specify the URL of the endpoint of the API server.
<2> Specify the name of provider `Secret` CR.
endif::[]

ifdef::vmware[]
[start=3]
. Create a `Host` manifest:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Host
metadata:
  name: <vmware_host>
  namespace: <namespace>
spec:
  provider:
    namespace: <namespace>
    name: <source_provider> <1>
  id: <source_host_mor> <2>
  ipAddress: <source_network_ip> <3>
EOF
----
<1> Specify the name of the VMware vSphere `Provider` CR.
<2> Specify the Managed Object Reference (moRef) of the VMware vSphere host. To retrieve the moRef, see xref:retrieving-vmware-moref_vmware[Retrieving a VMware vSphere moRef].
<3> Specify the IP address of the VMware vSphere migration network.

[start=4]
. Create a `NetworkMap` manifest to map the source and destination networks:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: NetworkMap
metadata:
  name: <network_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        name: <network_name>
        type: pod <1>
      source: <2>
        id: <source_network_id>
        name: <source_network_name>
    - destination:
        name: <network_attachment_definition> <3>
        namespace: <network_attachment_definition_namespace> <4>
        type: multus
      source:
        id: <source_network_id>
        name: <source_network_name>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
<1> Allowed values are `pod`, `multus`, and `ignored`. Use `ignored` to avoid attaching VMs to this network for this migration.
<2> You can use either the `id` or the `name` parameter to specify the source network. For `id`, specify the VMware vSphere network Managed Object Reference (moRef). To retrieve the moRef, see xref:retrieving-vmware-moref_vmware[Retrieving a VMware vSphere moRef].
<3> Specify a network attachment definition for each additional {virt} network.
<4> Required only when `type` is `multus`. Specify the namespace of the {virt} network attachment definition.
endif::[]

ifdef::rhv[]
[start=3]
. Create a `NetworkMap` manifest to map the source and destination networks:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: NetworkMap
metadata:
  name: <network_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        name: <network_name>
        type: pod <1>
      source: <2>
        id: <source_network_id>
        name: <source_network_name>
    - destination:
        name: <network_attachment_definition> <3>
        namespace: <network_attachment_definition_namespace> <4>
        type: multus
      source:
        id: <source_network_id>
        name: <source_network_name>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
<1> Allowed values are `pod` and `multus`.
<2> You can use either the `id` or the `name` parameter to specify the source network. For `id`, specify the {rhv-short} network Universal Unique ID (UUID).
<3> Specify a network attachment definition for each additional {virt} network.
<4> Required only when `type` is `multus`. Specify the namespace of the {virt} network attachment definition.
endif::[]

ifdef::ova[]
[start=3]
. Create a `NetworkMap` manifest to map the source and destination networks:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: NetworkMap
metadata:
  name: <network_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        name: <network_name>
        type: pod <1>
      source:
        id: <source_network_id> <2>
    - destination:
        name: <network_attachment_definition> <3>
        namespace: <network_attachment_definition_namespace> <4>
        type: multus
      source:
        id: <source_network_id>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
<1> Allowed values are `pod` and `multus`.
<2> Specify the OVA network Universal Unique ID (UUID).
<3> Specify a network attachment definition for each additional {virt} network.
<4> Required only when `type` is `multus`. Specify the namespace of the {virt} network attachment definition.
endif::[]

ifdef::ostack[]
[start=3]
. Create a `NetworkMap` manifest to map the source and destination networks:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: NetworkMap
metadata:
  name: <network_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        name: <network_name>
        type: pod <1>
      source:<2>
        id: <source_network_id>
        name: <source_network_name>
    - destination:
        name: <network_attachment_definition> <3>
        namespace: <network_attachment_definition_namespace> <4>
        type: multus
      source:
        id: <source_network_id>
        name: <source_network_name>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
<1> Allowed values are `pod` and `multus`.
<2> You can use either the `id` or the `name` parameter to specify the source network. For `id`, specify the {osp} network UUID.
<3> Specify a network attachment definition for each additional {virt} network.
<4> Required only when `type` is `multus`. Specify the namespace of the {virt} network attachment definition.
endif::[]

ifdef::cnv[]
[start=3]
. Create a `NetworkMap` manifest to map the source and destination networks:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: NetworkMap
metadata:
  name: <network_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        name: <network_name>
        type: pod <1>
      source:
        name: <network_name>
        type: pod
    - destination:
        name: <network_attachment_definition> <2>
        namespace: <network_attachment_definition_namespace> <3>
        type: multus
      source:
        name: <network_attachment_definition>
        namespace: <network_attachment_definition_namespace>
        type: multus
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
<1> Allowed values are `pod` and `multus`.
<2> Specify a network attachment definition for each additional {virt} network. Specify the
`namespace` either by using the `namespace property` or with a name built as follows: `<network_namespace>/<network_name>`.
<3> Required only when `type` is `multus`. Specify the namespace of the {virt} network attachment definition.
endif::[]

ifdef::vmware[]
[start=5]
. Create a `StorageMap` manifest to map source and destination storage:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: StorageMap
metadata:
  name: <storage_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        storageClass: <storage_class>
        accessMode: <access_mode> <1>
      source:
        id: <source_datastore> <2>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
<1> Allowed values are `ReadWriteOnce` and `ReadWriteMany`.
<2> Specify the VMware vSphere datastore moRef. For example, `f2737930-b567-451a-9ceb-2887f6207009`. To retrieve the moRef, see xref:retrieving-vmware-moref_vmware[Retrieving a VMware vSphere moRef].
endif::[]

ifdef::rhv[]
[start=4]
. Create a `StorageMap` manifest to map source and destination storage:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: StorageMap
metadata:
  name: <storage_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        storageClass: <storage_class>
        accessMode: <access_mode> <1>
      source:
        id: <source_storage_domain> <2>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
<1> Allowed values are `ReadWriteOnce` and `ReadWriteMany`.
<2> Specify the {rhv-short} storage domain UUID. For example, `f2737930-b567-451a-9ceb-2887f6207009`.
endif::[]

ifdef::ova[]
[start=4]
. Create a `StorageMap` manifest to map source and destination storage:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: StorageMap
metadata:
  name: <storage_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        storageClass: <storage_class>
        accessMode: <access_mode> <1>
      source:
        name:  Dummy storage for source provider <provider_name> <2>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
<1> Allowed values are `ReadWriteOnce` and `ReadWriteMany`.
<2> For OVA, the `StorageMap` can map only a single storage, which all the disks from the OVA are associated with, to a storage class at the destination. For this reason, the storage is referred to in the UI as "Dummy storage for source provider <provider_name>". In the YAML, write the phrase as it appears above, without the quotation marks and replacing <provider_name> with the actual name of the provider.
endif::[]

ifdef::ostack[]
[start=4]
. Create a `StorageMap` manifest to map source and destination storage:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: StorageMap
metadata:
  name: <storage_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        storageClass: <storage_class>
        accessMode: <access_mode> <1>
      source:
        id: <source_volume_type> <2>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
<1> Allowed values are `ReadWriteOnce` and `ReadWriteMany`.
<2> Specify the {osp} `volume_type` UUID. For example, `f2737930-b567-451a-9ceb-2887f6207009`.
endif::[]

ifdef::cnv[]
[start=4]
. Create a `StorageMap` manifest to map source and destination storage:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: StorageMap
metadata:
  name: <storage_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        storageClass: <storage_class>
        accessMode: <access_mode> <1>
      source:
        name: <storage_class>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
<1> Allowed values are `ReadWriteOnce` and `ReadWriteMany`.
endif::[]
+
. Optional: Create a `Hook` manifest to run custom code on a VM during the phase specified in the `Plan` CR:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Hook
metadata:
  name: <hook>
  namespace: <namespace>
spec:
  image: quay.io/kubev2v/hook-runner
  serviceAccount:<service account> <1>
  playbook: |
    LS0tCi0gbm... <2>
EOF
----
<1> Optional: {ocp} service account. Use the `serviceAccount` parameter to modify any cluster resources.
<2> Base64-encoded Ansible Playbook. If you specify a playbook, the `image` must include an `ansible-runner`.
+
[NOTE]
====
You can use the default `hook-runner` image or specify a custom image. If you specify a custom image, you do not have to specify a playbook.
====

ifdef::vmware[]
[start=7]
. Enter the following command to create the network attachment definition (NAD) of the transfer network used for {project-short} migrations.
+
You use this definition to configure an IP address for the interface, either from the Dynamic Host Configuration Protocol (DHCP) or statically.
+
Configuring the IP address enables the interface to reach the configured gateway.
+
[source,yaml,subs="attributes+"]
----
$ oc edit NetworkAttachmentDefinitions <name_of_the_NAD_to_edit>
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: <name_of_transfer_network>
  namespace: <namespace>
  annotations:
    forklift.konveyor.io/route: <IP_address>
----

. Create a `Plan` manifest for the migration:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Plan
metadata:
  name: <plan> <1>
  namespace: <namespace>
spec:
  warm: false <2>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
  map: <3>
    network: <4>
      name: <network_map> <5>
      namespace: <namespace>
    storage: <6>
      name: <storage_map> <7>
      namespace: <namespace>
  preserveStaticIPs: <8>
  networkNameTemplate: <network_interface_template> <9>
  pvcNameTemplate: <pvc_name_template> <10>
  pvcNameTemplateUseGenerateName: true <11>
  targetNamespace: <target_namespace>
  volumeNameTemplate: <volume_name_template> <12>
  vms: <13>
    - id: <source_vm1> <14>
    - name: <source_vm2>
      networkNameTemplate: <network_interface_template_for_this_vm> <15>
      pvcNameTemplate: <pvc_name_template_for_this_vm> <16>
      volumeNameTemplate: <volume_name_template_for_this_vm> <17>
      targetName: <target_name> <18>
      hooks: <19>
        - hook:
            namespace: <namespace>
            name: <hook> <20>
          step: <step> <21>

EOF
----
<1> Specify the name of the `Plan` CR.
<2> Specify whether the migration is warm - `true` - or cold - `false`. If you specify a warm migration without specifying a value for the `cutover` parameter in the `Migration` manifest, only the precopy stage will run.
<3> Specify only one network map and one storage map per plan.
<4> Specify a network mapping even if the VMs to be migrated are not assigned to a network. The mapping can be empty in this case.
<5> Specify the name of the `NetworkMap` CR.
<6> Specify a storage mapping even if the VMs to be migrated are not assigned with disk images. The mapping can be empty in this case.
<7> Specify the name of the `StorageMap` CR.
<8> By default, virtual network interface controllers (vNICs) change during the migration process. As a result, vNICs that are configured with a static IP address linked to the interface name in the guest VM lose their IP address. To avoid this, set `preserveStaticIPs` to `true`. {project-short} issues a warning message about any VMs for which vNIC properties are missing. To retrieve any missing vNIC properties, run those VMs in vSphere in order for the vNIC properties to be reported to {project-short}.
<9> [[callout9]]Optional. Specify a template for the network interface name for the VMs in your plan.
The template follows the Go template syntax and has access to the following variables:
* `.NetworkName:` If the target network is `multus`, add the name of the Multus Network Attachment Definition. Otherwise, leave this variable empty.
* `.NetworkNamespace`: If the target network is `multus`, add the namespace where the Multus Network Attachment Definition is located.
* `.NetworkType`: Specifies the network type. Options: `multus` or `pod`.
* `.NetworkIndex`: Sequential index of the network interface (0-based).
+
*Examples*
* `"net-{{.NetworkIndex}}"`
* `{{if eq .NetworkType "pod"}}pod{{else}}multus-{{.NetworkIndex}}{{end}}"`
+
Variable names cannot exceed 63 characters. This rule applies to a network name network template, a PVC name template, a VM name template, and a volume name template.
<10> [[callout10]]Optional. Specify a template for the persistent volume claim (PVC) name for a plan.
The template follows the Go template syntax and has access to the following variables:
* `.VmName`: Name of the VM.
* `.PlanName`: Name of the migration plan.
* `.DiskIndex`: Initial volume index of the disk.
* `.RootDiskIndex`: Index of the root disk.
* `.Shared`: Options: `true`, for a shared volume, `false`, for a non-shared volume.
+
*Examples*
* `"{{.VmName}}-disk-{{.DiskIndex}}"`
* `"{{if eq .DiskIndex .RootDiskIndex}}root{{else}}data{{end}}-{{.DiskIndex}}"`
* `"{{if .Shared}}shared-{{end}}{{.VmName}}-{{.DiskIndex}}"`
+
<11> Optional:
* When set to `true`, {project-short} adds one or more randonly generated alphanumeric characters to the name of the PVC in order to ensure all PVCs have unique names.
* When set to `false`, if you specify a `pvcNameTemplate`, {project-short} does not add such charchters to the name of the PVC.
+
include::snip_pvcNameTemplateUseGenerateName-warn.adoc[]
+
<12> [[callout12]]Optional: Specify a template for the volume interface name for the VMs in your plan.
The template follows the Go template syntax and has access to the following variables:
** `.PVCName`: Name of the PVC mounted to the VM using this volume.
** `.VolumeIndex`: Sequential index of the volume interface (0-based).
+
*Examples*
** `"disk-{{.VolumeIndex}}"`
** `"pvc-{{.PVCName}}"`
<13> You can use either the `id` or the `name` parameter to specify the source VMs.
<14> Specify the VMware vSphere VM moRef. To retrieve the moRef, see xref:retrieving-vmware-moref_vmware[Retrieving a VMware vSphere moRef].
<15> Optional: Specify a network interface name for the specific VM. Overrides the value set in `spec:networkNameTemplate`. Variables and examples as in xref:callout9[callout 9].
<16> Optional: Specify a PVC name for the specific VM. Overrides the value set in `spec:pvcNameTemplate`. Variables and examples as in xref:callout10[callout 10].
<17> Optional: Specify a volume name for the specific VM. Overrides the value set in `spec:volumeNameTemplate`. Variables and examples as in xref:callout12[callout 12].
<18> Optional: {project-short} automatically generates a name for the target VM. You can override this name by using this parameter and entering a new name. The name you enter must be unique and it must be a valid Kubernetes subdomain. Otherwise, the migration fails automatically.
<19> Optional: Specify up to two hooks for a VM. Each hook must run during a separate migration step.
<20> Specify the name of the `Hook` CR.
<21> Allowed values are `PreHook`, before the migration plan starts, or `PostHook`, after the migration is complete.
+
include::snip_vmware-name-change.adoc[]
endif::[]

ifdef::rhv[]
[start=6]
. Enter the following command to create the network attachment definition (NAD) of the transfer network used for {project-short} migrations.
+
You use this definition to configure an IP address for the interface, either from the Dynamic Host Configuration Protocol (DHCP) or statically.
+
Configuring the IP address enables the interface to reach the configured gateway.
+
[source,yaml,subs="attributes+"]
----
$ oc edit NetworkAttachmentDefinitions <name_of_the_NAD_to_edit>
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: <name_of_transfer_network>
  namespace: <namespace>
  annotations:
    forklift.konveyor.io/route: <IP_address>
----

. Create a `Plan` manifest for the migration:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Plan
metadata:
  name: <plan> <1>
  namespace: <namespace>
  preserveClusterCpuModel: true <2>
spec:
  warm: false <3>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
  map: <4>
    network: <5>
      name: <network_map> <6>
      namespace: <namespace>
    storage: <7>
      name: <storage_map> <8>
      namespace: <namespace>
  targetNamespace: <target_namespace>
  vms: <9>
    - id: <source_vm1> <10>
    - name: <source_vm2>
      hooks: <11>
        - hook:
            namespace: <namespace>
            name: <hook> <12>
          step: <step> <13>
EOF
----
<1> Specify the name of the `Plan` CR.
<2> See note below.
<3> Specify whether the migration is warm or cold. If you specify a warm migration without specifying a value for the `cutover` parameter in the `Migration` manifest, only the precopy stage will run.
<4> Specify only one network map and one storage map per plan.
<5> Specify a network mapping even if the VMs to be migrated are not assigned to a network. The mapping can be empty in this case.
<6> Specify the name of the `NetworkMap` CR.
<7> Specify a storage mapping even if the VMs to be migrated are not assigned with disk images. The mapping can be empty in this case.
<8> Specify the name of the `StorageMap` CR.
<9> You can use either the `id` or the `name` parameter to specify the source VMs.
<10> Specify the {rhv-short} VM UUID.
<11> Optional: Specify up to two hooks for a VM. Each hook must run during a separate migration step.
<12> Specify the name of the `Hook` CR.
<13> Allowed values are `PreHook`, before the migration plan starts, or `PostHook`, after the migration is complete.
+
[NOTE]
====
* If the migrated machine is set with a custom CPU model, it will be set with that CPU model in the destination cluster, regardless of the setting of `preserveClusterCpuModel`.

* If the migrated machine is _not_ set with a custom CPU model:

** If `preserveClusterCpuModel` is set to 'true`, {project-short} checks the CPU model of the VM when it runs in {rhv-short}, based on the cluster's configuration, and then sets the migrated VM with that CPU model.
** If `preserveClusterCpuModel` is set to 'false`, {project-short} does not set a CPU type and the VM is set with the default CPU model of the destination cluster.
====
endif::[]

ifdef::ova[]
[start=6]
. Enter the following command to create the network attachment definition (NAD) of the transfer network used for {project-short} migrations.
+
You use this definition to configure an IP address for the interface, either from the Dynamic Host Configuration Protocol (DHCP) or statically.
+
Configuring the IP address enables the interface to reach the configured gateway.
+
[source,yaml,subs="attributes+"]
----
$ oc edit NetworkAttachmentDefinitions <name_of_the_NAD_to_edit>
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: <name_of_transfer_network>
  namespace: <namespace>
  annotations:
    forklift.konveyor.io/route: <IP_address>
----

. Create a `Plan` manifest for the migration:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Plan
metadata:
  name: <plan> <1>
  namespace: <namespace>
spec:
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
  map: <2>
    network: <3>
      name: <network_map> <4>
      namespace: <namespace>
    storage: <5>
      name: <storage_map> <6>
      namespace: <namespace>
  targetNamespace: <target_namespace>
  vms: <7>
    - id: <source_vm1> <8>
    - name: <source_vm2>
      hooks: <9>
        - hook:
            namespace: <namespace>
            name: <hook> <10>
          step: <step> <11>
EOF
----
<1> Specify the name of the `Plan` CR.
<2> Specify only one network map and one storage map per plan.
<3> Specify a network mapping, even if the VMs to be migrated are not assigned to a network. The mapping can be empty in this case.
<4> Specify the name of the `NetworkMap` CR.
<5> Specify a storage mapping even if the VMs to be migrated are not assigned with disk images. The mapping can be empty in this case.
<6> Specify the name of the `StorageMap` CR.
<7> You can use either the `id` or the `name` parameter to specify the source VMs.
<8> Specify the OVA VM UUID.
<9> Optional: You can specify up to two hooks for a VM. Each hook must run during a separate migration step.
<10> Specify the name of the `Hook` CR.
<11> Allowed values are `PreHook`, before the migration plan starts, or `PostHook`, after the migration is complete.
endif::[]

ifdef::ostack[]
[start=6]
. Enter the following command to create the network attachment definition (NAD) of the transfer network used for {project-short} migrations.
+
You use this definition to configure an IP address for the interface, either from the Dynamic Host Configuration Protocol (DHCP) or statically.
+
Configuring the IP address enables the interface to reach the configured gateway.
+
[source,yaml,subs="attributes+"]
----
$ oc edit NetworkAttachmentDefinitions <name_of_the_NAD_to_edit>
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: <name_of_transfer_network>
  namespace: <namespace>
  annotations:
    forklift.konveyor.io/route: <IP_address>
----

. Create a `Plan` manifest for the migration:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Plan
metadata:
  name: <plan> <1>
  namespace: <namespace>
spec:
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
  map: <2>
    network: <3>
      name: <network_map> <4>
      namespace: <namespace>
    storage: <5>
      name: <storage_map> <6>
      namespace: <namespace>
  targetNamespace: <target_namespace>
  vms: <7>
    - id: <source_vm1> <8>
    - name: <source_vm2>
      hooks: <9>
        - hook:
            namespace: <namespace>
            name: <hook> <10>
          step: <step> <11>
EOF
----
<1> Specify the name of the `Plan` CR.
<2> Specify only one network map and one storage map per plan.
<3> Specify a network mapping, even if the VMs to be migrated are not assigned to a network. The mapping can be empty in this case.
<4> Specify the name of the `NetworkMap` CR.
<5> Specify a storage mapping, even if the VMs to be migrated are not assigned with disk images. The mapping can be empty in this case.
<6> Specify the name of the `StorageMap` CR.
<7> You can use either the `id` or the `name` parameter to specify the source VMs.
<8> Specify the {osp} VM UUID.
<9> Optional: Specify up to two hooks for a VM. Each hook must run during a separate migration step.
<10> Specify the name of the `Hook` CR.
<11> Allowed values are `PreHook`, before the migration plan starts, or `PostHook`, after the migration is complete.
endif::[]

ifdef::cnv[]
[start=6]
. Enter the following command to create the network attachment definition (NAD) of the transfer network used for {project-short} migrations.
+
You use this definition to configure an IP address for the interface, either from the Dynamic Host Configuration Protocol (DHCP) or statically.
+
Configuring the IP address enables the interface to reach the configured gateway.
+
[source,yaml,subs="attributes+"]
----
$ oc edit NetworkAttachmentDefinitions <name_of_the_NAD_to_edit>
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: <name_of_transfer_network>
  namespace: <namespace>
  annotations:
    forklift.konveyor.io/route: <IP_address>
----

. Create a `Plan` manifest for the migration:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Plan
metadata:
  name: <plan> <1>
  namespace: <namespace>
spec:
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
  map: <2>
    network: <3>
      name: <network_map> <4>
      namespace: <namespace>
    storage: <5>
      name: <storage_map> <6>
      namespace: <namespace>
  targetNamespace: <target_namespace>
  vms:
    - name: <source_vm>
      namespace: <namespace>
      hooks: <7>
        - hook:
            namespace: <namespace>
            name: <hook> <8>
          step: <step> <9>
EOF
----
<1> Specify the name of the `Plan` CR.
<2> Specify only one network map and one storage map per plan.
<3> Specify a network mapping, even if the VMs to be migrated are not assigned to a network. The mapping can be empty in this case.
<4> Specify the name of the `NetworkMap` CR.
<5> Specify a storage mapping, even if the VMs to be migrated are not assigned with disk images. The mapping can be empty in this case.
<6> Specify the name of the `StorageMap` CR.
<7> Optional: Specify up to two hooks for a VM. Each hook must run during a separate migration step.
<8> Specify the name of the `Hook` CR.
<9> Allowed values are `PreHook`, before the migration plan starts, or `PostHook`, after the migration is complete.
endif::[]

. Create a `Migration` manifest to run the `Plan` CR:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Migration
metadata:
  name: <name_of_migration_cr>
  namespace: <namespace>
spec:
  plan:
    name: <name_of_plan_cr>
    namespace: <namespace>
  cutover: <optional_cutover_time>
EOF
----
+
[NOTE]
====
If you specify a cutover time, use the ISO 8601 format with the UTC time offset, for example, `2024-04-04T01:23:45.678+09:00`.
====

