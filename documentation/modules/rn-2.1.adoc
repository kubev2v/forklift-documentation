// Module included in the following assemblies:
//
// * documentation/doc-Release_notes/master.adoc

:_content-type: context
[id="rn-21_{context}"]
= {project-full} 2.1

You can migrate virtual machines (VMs) from VMware vSphere or {rhv-full} to {virt} with {the-lc} {project-first}.

The release notes describe new features and enhancements, known issues, and technical changes.

[id="technical-changes-21_{context}"]
== Technical changes

.VDDK image added to `HyperConverged` custom resource

The VMware Virtual Disk Development Kit (VDDK) SDK image must be added to the `HyperConverged` custom resource. Before this release, it was referenced in the `v2v-vmware` config map.

[id="new-features-and-enhancements-21_{context}"]
== New features and enhancements

This release adds the following features and improvements.

.Cold migration from {rhv-full}

You can perform a cold migration of VMs from {rhv-full}.

.Migration hooks

You can create migration hooks to run Ansible playbooks or custom code before or after migration.

.Filtered `must-gather` data collection

You can specify options for the `must-gather` tool that enable you to filter the data by namespace, migration plan, or VMs.

.SR-IOV network support

You can migrate VMs with a single root I/O virtualization (SR-IOV) network interface if the {virt} environment has an SR-IOV network.

[id="known-issues-21_{context}"]
== Known issues

.QEMU guest agent is not installed on migrated VMs

The QEMU guest agent is not installed on migrated VMs. Workaround: Install the QEMU guest agent with a post-migration hook. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2018062[*BZ#2018062*])

.Disk copy stage does not progress

The disk copy stage of a {rhv-short} VM does not progress and the {project-short} web console does not display an error message. link:https://bugzilla.redhat.com/show_bug.cgi?id=1990596[(*BZ#1990596*)]

The cause of this problem might be one of the following conditions:

* The storage class does not exist on the target cluster.
* The VDDK image has not been added to the `HyperConverged` custom resource.
* The VM does not have a disk.
* The VM disk is locked.
* The VM time zone is not set to UTC.
* The VM is configured for a USB device.

To disable USB devices, see link:https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/virtual_machine_management_guide/index#sect-Configuring_USB_Devices[Configuring USB Devices] in the Red Hat Virtualization documentation.

To determine the cause:

. Click *Workloads* -> *Virtualization* in the {ocp} web console.
. Click the *Virtual Machines* tab.
. Select a virtual machine to open the *Virtual Machine Overview* screen.
. Click *Status* to view the status of the virtual machine.

.VM time zone must be UTC with no offset

The time zone of the source VMs must be UTC with no offset. You can set the time zone to `GMT Standard Time` after first assessing the potential impact on the workload. link:https://bugzilla.redhat.com/show_bug.cgi?id=1993259[(*BZ#1993259*)]

.{rhv-short} resource UUID causes a "Provider not found" error

If a {rhv-short} resource UUID is used in a `Host`, `NetworkMap`, `StorageMap`, or `Plan` custom resource (CR), a "Provider not found" error is displayed.

You must use the resource name. link:https://bugzilla.redhat.com/show_bug.cgi?id=1994037[(*BZ#1994037*)]

.Same {rhv-short} resource name in different data centers causes ambiguous reference

If a {rhv-short} resource name is used in a `NetworkMap`, `StorageMap`, or `Plan` custom resource (CR) and if the same resource name exists in another data center, the `Plan` CR displays a critical "Ambiguous reference" condition. You must rename the resource or use the resource UUID in the CR.

In the web console, the resource name appears twice in the same list without a data center reference to distinguish them. You must rename the resource. link:https://bugzilla.redhat.com/show_bug.cgi?id=1993089[(*BZ#1993089*)]

.Snapshots are not deleted after warm migration
// only CNV 4.8.2. Resolved in CNV 4.8.3
Snapshots are not deleted automatically after a successful warm migration of a VMware VM. You must delete the snapshots manually in VMware vSphere. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2001270[*BZ#2001270*])
