// Module included in the following assemblies:
//
// * documentation/Migrating_your_virtual_machines/assemblies/assembly_migrating-from-cnv.adoc

:_mod-docs-content-type: PROCEDURE
[id="migrating-live-cnv-cnv-vms-cli_{context}"]

= Running a Red Hat {virt} live migration from the command-line

[role="_abstract"]
You can perform a live migration by using the command-line interface (CLI). The procedure for live migration is identical to the procedure for other migrations between {virt} clusters except for the addition of the `type` label in the `Plan` CR. For a live migration, the `type` label must be set to `live`.

[IMPORTANT]
====
Live migration is a Technology Preview feature only. Technology Preview features
are not supported with Red Hat production service level agreements (SLAs) and
might not be functionally complete. Red Hat does not recommend using them
in production. These features provide early access to upcoming product
features, enabling customers to test functionality and provide feedback during
the development process.
For more information about the support scope of Red Hat Technology Preview
features, see https://access.redhat.com/support/offerings/techpreview/.
====

.Prerequisites
As described in the prerequisites for live migration. For more information, see link:https://docs.redhat.com/en/documentation/migration_toolkit_for_virtualization/2.10/html/planning_your_migration_to_red_hat_openshift_virtualization/assembly_provider-specific-requirements-for-migration_mtv#cnv-cnv-live-prerequisites_mtv[{virt} live migration prerequisites].

.Procedure
. Create a `Secret` manifest for the source provider credentials:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: <secret>
  namespace: <namespace>
  ownerReferences: 
    - apiVersion: forklift.konveyor.io/v1beta1
      kind: Provider
      name: <provider_name>
      uid: <provider_uid>
  labels:
    createdForProviderType: openshift
    createdForResourceType: providers
type: Opaque
stringData:
  token: <token>
  password: <password> 
  insecureSkipVerify: <"true"/"false">
  cacert: |
    <ca_certificate>
  url: <api_end_point>
EOF
----
+
where:

`ownerReferences`::
Is an optional section in which you can specify a provider's `name` and `uid`.

`<token>`::
Specifies a token for a service account with `cluster-admin` privileges. If both `token` and `url` are left blank, the local {ocp-short} cluster is used.

`<password>`::
Specifies the user password.

`<"true"/"false">`::
Specifies `"true"` to skip certificate verification, and specifies `"false"` to verify the certificate. Defaults to `"false"` if not specified. Skipping certificate verification proceeds with an insecure migration and then the certificate is not required. Insecure migration means that the transferred data is sent over an insecure connection and potentially sensitive data could be exposed.

`cacert`::
Specifies the CA cert object. When this field is not set and 'skip certificate verification' is disabled, {project-short} attempts to use the system CA.

`<api_end_point>`::
Specifies the URL of the endpoint of the API server.

. Create a `Provider` manifest for the source provider:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Provider
metadata:
  name: <source_provider>
  namespace: <namespace>
spec:
  type: openshift
  url: <api_end_point>
  secret:
    name: <secret>
    namespace: <namespace>
EOF
----
+
where:

`<api_end_point>`::
Specifies the URL of the endpoint of the API server.

`<secret>`::
Specifies the name of the provider `Secret` CR.

. Create a `NetworkMap` manifest to map the source and destination networks:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: NetworkMap
metadata:
  name: <network_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        name: <network_name>
        type: pod
      source:
        name: <network_name>
        type: pod
    - destination:
        name: <network_attachment_definition>
        namespace: <network_attachment_definition_namespace>
        type: multus
      source:
        name: <network_attachment_definition>
        namespace: <network_attachment_definition_namespace>
        type: multus
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
+
where:

`type`::
Specifies the network type. Allowed values are `pod`, `ignored`, and `multus`.

`<network_attachment_definition>`::
Specifies the network name. When the `type` is `multus`, use the name of the {virt} network attachment (NAD) definition.

`<network_attachment_definition_namespace>`::
Specifies the namespace of the {virt} NAD. Required only when the `type` is `multus`. 

. Create a `StorageMap` manifest to map source and destination storage:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: StorageMap
metadata:
  name: <storage_map>
  namespace: <namespace>
spec:
  map:
    - destination:
        storageClass: <storage_class>
        accessMode: <access_mode>
      source:
        name: <storage_class>
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
EOF
----
+
where:

`<access_mode>`::
Specifies the access mode. Allowed values are `ReadWriteOnce` and `ReadWriteMany`.

. Optional: Create a `Hook` manifest to run custom code on a VM during the phase specified in the `Plan` CR:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Hook
metadata:
  name: <hook>
  namespace: <namespace>
spec:
  image: quay.io/kubev2v/hook-runner
  serviceAccount: <service_account>
  playbook: |
    LS0tCi0gbm...
EOF
----
+
where:

`serviceAccount`::
Specifies the {ocp} service account. This is an optional label. Use the `serviceAccount` parameter to modify any cluster resources.

`playbook`::
Specifies the Base64-encoded Ansible Playbook. If you specify a playbook, the `image` must include an `ansible-runner`.
+
[NOTE]
====
You can use the default `hook-runner` image or specify a custom image. If you specify a custom image, you do not have to specify a playbook.
====

. Create a `Plan` manifest for the migration:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Plan
metadata:
  name: <plan>
  namespace: <namespace>
spec:
  provider:
    source:
      name: <source_provider>
      namespace: <namespace>
    destination:
      name: <destination_provider>
      namespace: <namespace>
  map:
    network:
      name: <network_map>
      namespace: <namespace>
    storage:
      name: <storage_map>
      namespace: <namespace>
  type: live
  targetNamespace: <target_namespace>
  vms:
    - name: <source_vm>
      namespace: <namespace>
      hooks:
        - hook:
            namespace: <namespace>
            name: <hook>
          step: <step>
EOF
----
+
where:

`<plan>`::
Specifies the name of the `Plan` CR.

`map`::
Specifies only one network map and one storage map per plan.

`network`::
Specifies a network mapping, even if the VMs to be migrated are not assigned to a network. The mapping can be empty in this case.

`<network_map>`::
Specifies the name of the `NetworkMap` CR.

`storage`::
Specifies a storage mapping, even if the VMs to be migrated are not assigned with disk images. The mapping can be empty in this case.

`<storage_map>`::
Specifies the name of the `StorageMap` CR.

`type`::
Specifies the type of migration. Must be set to `live`.

`hooks`::
Specifies up to two hooks for a VM. Each hook must run during a separate migration step. This is an optional label.

`<hook>`::
Specifies the name of the `Hook` CR.

`<step>`::
Specifies the hook step. Allowed values are `PreHook`, before the migration plan starts, or `PostHook`, after the migration is complete.

. Create a `Migration` manifest to run the `Plan` CR:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | {oc} apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Migration
metadata:
  name: <name_of_migration_cr>
  namespace: <namespace>
spec:
  plan:
    name: <name_of_plan_cr>
    namespace: <namespace>
EOF
----
+
[NOTE]
====
The `cutover` field is irrelevant for live migrations, so it is not included in the `Migration` CR of this procedure. 
====
